{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'QUAL_PALETTES' from 'seaborn.palettes' (C:\\Users\\d4i7v\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\palettes.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelational\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\relational.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_oldcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     VectorPlotter,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     locator_to_legend_entries,\n\u001b[0;32m     13\u001b[0m     adjust_legend_subtitles,\n\u001b[0;32m     14\u001b[0m     _default_color,\n\u001b[0;32m     15\u001b[0m     _deprecate_ci,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_statistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EstimateAggregator\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     share_init_params_with_map,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     QUAL_PALETTES,\n\u001b[0;32m     20\u001b[0m     color_palette,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     _check_argument,\n\u001b[0;32m     24\u001b[0m     get_color_cycle,\n\u001b[0;32m     25\u001b[0m     remove_na,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSemanticMapping\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'QUAL_PALETTES' from 'seaborn.palettes' (C:\\Users\\d4i7v\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\palettes.py)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Acquire, preprocess, and analyze the data\n",
    "\n",
    "The essential subtasks for this part of the project are:\n",
    "1. Download the datasets (noting the correct subsets to use, as discussed above).\n",
    "2. Load the datasets into numpy objects (i.e., arrays or matrices) in Python. Remember to convert the wine dataset\n",
    "to a binary task, as discussed above.\n",
    "3. Clean the data. Are there any missing or malformed features? Are there are other data oddities that need to be\n",
    "dealt with? You should remove any examples with missing or malformed features and note this in your\n",
    "report.\n",
    "4. Compute some statistics on the data. E.g., what are the distributions of the positive vs. negative classes, what\n",
    "are the distributions of some of the numerical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Dataset: Breast Cancer Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('breast-cancer-wisconsin.data', header=None)\n",
    "data.columns=['Id number','Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']\n",
    "data2 = data.apply(pd.to_numeric, errors='coerce') #Convert argument to numeric type and invalid parsing is set as NaN\n",
    "data = data2.dropna() #Rewrrite previous data entry with new complete data \n",
    "\n",
    "#Patient Number is not required\n",
    "del data['Id number']\n",
    "\n",
    "# Display the first few records\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outlier data\n",
    "\n",
    "Using interquartile range to calculate and remove outliers.\n",
    "Using Features \"Bare Nuclei\" and \"Cell Size\" to remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from quality column. Find data points with extreme high or low values\n",
    "feature = \"Bare Nuclei\"\n",
    "\n",
    "# Calculate the 25th and 75th percentile of the column\n",
    "Q1, Q3 = np.percentile(data[feature], q=25), np.percentile(data[feature], q=75) \n",
    "\n",
    "# Select and Calculate outlier range using interquartile range\n",
    "outlier_range =   (Q3 - Q1)\n",
    "\n",
    "# Outliers in data\n",
    "num_data = data[((data[\"Bare Nuclei\"] < (Q1 - outlier_range)) | (data[\"Bare Nuclei\"] > (Q3 + outlier_range)))].shape[0]\n",
    "\n",
    "# Remove outliers outside the outlier range\n",
    "data = data[~((data[\"Bare Nuclei\"] < (Q1 - outlier_range)) | (data[\"Bare Nuclei\"] > (Q3 + outlier_range)))].reset_index(drop = True)\n",
    "\n",
    "# Display number of outliers\n",
    "print(\"Number of outliers removed: \", num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from quality column. Find data points with extreme high or low values\n",
    "feature = \"Uniformity of Cell Size\"\n",
    "\n",
    "# Calculate the 25th and 75th percentile of the column\n",
    "Q1, Q3 = np.percentile(data[feature], q=25), np.percentile(data[feature], q=75) \n",
    "\n",
    "# Select and Calculate outlier range using interquartile range\n",
    "outlier_range =   (Q3 - Q1)\n",
    "\n",
    "# Outliers in data\n",
    "num_data = data[((data[\"Uniformity of Cell Size\"] < (Q1 - outlier_range)) | (data[\"Uniformity of Cell Size\"] > (Q3 + outlier_range)))].shape[0]\n",
    "\n",
    "# Remove outliers outside the outlier range\n",
    "data = data[~((data[\"Uniformity of Cell Size\"] < (Q1 - outlier_range)) | (data[\"Uniformity of Cell Size\"] > (Q3 + outlier_range)))].reset_index(drop = True)\n",
    "\n",
    "# Display number of outliers\n",
    "print(\"Number of outliers removed: \", num_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update target variable from Categorical (2,4) to Categorical (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using lambda function to change values in the Class column\n",
    "data[\"Class\"]=1*(data[\"Class\"]>3)\n",
    "\n",
    "#Display the changed records\n",
    "data[[\"Class\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking number of total breast cancer data and percentage of benign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of entries in a cloumn\n",
    "num_of_patients = data.shape[0]\n",
    "\n",
    "# Number of benign breast cancer data\n",
    "benign_number=data.loc[(data['Class']==0)] #Accessing Column using label\n",
    "number_benign=benign_number.shape[0] #Returns Dimensionality of DataFrame in tuple format\n",
    "\n",
    "# Number of malignant breast cancer data\n",
    "malignant_number=data.loc[(data['Class']==1)]\n",
    "number_malignant=malignant_number.shape[0]\n",
    "\n",
    "# Percentage of benign Class data\n",
    "benign_percentage = number_benign*100/num_of_patients\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of patients: {}\".format(num_of_patients))\n",
    "print(\"Number of benign tumor patient: {}\".format(number_benign))\n",
    "print(\"Number of malignant tumor patient: {}\".format(number_malignant))\n",
    "print(\"Percentage of benign tumor patient: {:.2f}%\".format(benign_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Pie Chart to show distribution of benign and malignant classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to plot\n",
    "plt.figure(figsize = (8,8))\n",
    "labels = 'Class = Benign', 'Class = Malignant'\n",
    "plt.title('Number of Patients vs Tumor Class')\n",
    "sizes = [number_benign, number_malignant]\n",
    "colors = ['lightgreen', 'red']\n",
    "\n",
    "# Plot pie chart\n",
    "plt.pie(sizes,  labels=labels, colors=colors,autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Individual Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Clump Thickness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Uniformity of Cell Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Uniformity of Cell Shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Marginal Adhesion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Single Epithelial Cell Size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Bare Nuclei'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Bland Chromatin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Normal Nucleoli'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data['Mitoses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix output shows following behaviour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data.corr()\n",
    "plt.figure(figsize=(12, 12))\n",
    "heatmap = sns.heatmap(correlation, annot=True, linewidths=0.5, linecolor=\"white\",vmin=0.3, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test split\n",
    "\n",
    "# Select 80% for training\n",
    "data_train = data.sample(frac = 0.8)\n",
    "x_train = np.array(data_train.iloc[:, :-1])\n",
    "y_train = np.array(data_train[\"Class\"])\n",
    "\n",
    "# Select rest of data for testing\n",
    "data_test = data.drop(data_train.index)\n",
    "x_test = np.array(data_test.iloc[:, :-1])\n",
    "y_test = np.array(data_test[\"Class\"])\n",
    "\n",
    "# Show number of training and testing data points\n",
    "print(\"Number of Training data points: \", len(x_train))\n",
    "print(\"Number of Testing data points: \", len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2:  Implementing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation - Logistic regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sigmoid over a matrix of values (element wise)\n",
    "sigma = lambda y: np.array(list(map(lambda x: 1/(1+np.exp(-x)), y)))\n",
    "\n",
    "'''\n",
    "LogisticRegression Class:\n",
    "This class implements logistic regression model with following attributes and methods:\n",
    "  Attributes:\n",
    "    1. Weights matrix\n",
    "    2. Mean of Training data\n",
    "    3. Standard Deviation of Training data\n",
    "  Methods:\n",
    "    1. __init__: Initialize weights matrix\n",
    "    2. normalize_data: Normalizes data using Training data's mean and standard deviation\n",
    "    3. fit: Perform training on Train dataset.\n",
    "    4. predict: Perform testing using Test dataset \n",
    "'''\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, initial_weights):\n",
    "        '''\n",
    "        Constructor\n",
    "         Inputs: Initial weights\n",
    "        '''\n",
    "        \n",
    "        self.w = initial_weights\n",
    "        self.x_train_mean = np.nan\n",
    "        self.x_train_std = np.nan\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def normalize_data(self, x, from_train = False):\n",
    "        '''    \n",
    "        Function to Normalize data\n",
    "         Inputs:\n",
    "          x: Input data (Training or Testing dataset)\n",
    "          from_train: If called from training code (True), the mean and Standard Deviation are noted for use with Test data.\n",
    "         Outputs:\n",
    "          Normalized data\n",
    "        '''\n",
    "        \n",
    "        # Save Training mean and SD.\n",
    "        # Test data must be normalized using Training Mean and Training SD,\n",
    "        # because of the i.i.d assumption\n",
    "        if from_train:\n",
    "            self.x_train_mean = np.mean(x, axis = 0)\n",
    "            self.x_train_std = np.std(x,axis = 0)\n",
    "        \n",
    "        # Return normalized values\n",
    "        return (x - self.x_train_mean)/self.x_train_std\n",
    "    \n",
    "    \n",
    "    def fit(self, x_train, y_train, alpha, iters, stop_criteria, normalize = True):\n",
    "        '''    \n",
    "        Function to perform Training\n",
    "         Inputs: \n",
    "          x_train: Training data - input features\n",
    "          y_train: Training data - targets\n",
    "          alpha: Learning rate\n",
    "          iters: Maximum number of Iterations if not converged\n",
    "          stop_criteria: Lowest allowed norm of \"change in weights\" (Stopping criteria)\n",
    "          normalize: should data be normalized?\n",
    "         Output:\n",
    "          List of cross entropy losses\n",
    "\n",
    "        '''\n",
    "        \n",
    "        # Normalize data if requested. Save the Training mean and SD\n",
    "        if normalize:\n",
    "            x_train = self.normalize_data(x_train, from_train = True)\n",
    "        \n",
    "        # Convert y_train to 2D matrix\n",
    "        y_train = y_train[np.newaxis]\n",
    "        \n",
    "        # Maintain a list of cross entropy losses\n",
    "        ce_losses = []\n",
    "        \n",
    "        # Small value to prevent Log(0) situation\n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        # Iterations of Weight updates\n",
    "        for iterate in range(iters):\n",
    "            \n",
    "            # Cross Entropy calculation\n",
    "            ce = -1*(\\\n",
    "                     y_train.dot(np.log(sigma(self.w.T.dot(x_train.T)).T+epsilon)) + \\\n",
    "                     (1 - y_train).dot(np.log(1-sigma(self.w.T.dot(x_train.T)).T+epsilon))\\\n",
    "                    )\n",
    "            \n",
    "            # Gradient calc - Change in weights\n",
    "            deltaW = x_train.T.dot((y_train - sigma(self.w.T.dot(x_train.T))).T)\n",
    "            \n",
    "            # Update list of losses\n",
    "            ce_losses = ce_losses + ce.flatten().tolist()\n",
    "            \n",
    "            # Weight update equation\n",
    "            self.w += alpha*deltaW\n",
    "            \n",
    "            # Check if norm of deltaW is lower than stopping criteria\n",
    "            # if yes, stop\n",
    "            if np.linalg.norm(deltaW) < stop_criteria:\n",
    "                print(\"Number of iterations: \", iterate)\n",
    "                break\n",
    "        return ce_losses\n",
    "\n",
    "    \n",
    "    def predict(self, x_test, normalize = True):\n",
    "        '''\n",
    "        Function to predict classes using given input features\n",
    "         Inputs:\n",
    "          x_test: Test dataset input features\n",
    "          normalize: Should data be normalized?\n",
    "         Outputs:\n",
    "          y_pred: Predicted classes corresponding to input records\n",
    "        '''\n",
    "        \n",
    "        if normalize:\n",
    "            x_test = self.normalize_data(x_test)\n",
    "        \n",
    "        # Calculate predicted class\n",
    "        y_pred = 1*(self.w.T.dot(x_test.T) > 0.5)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "# Function to evaluate accuracies\n",
    "def evaluate_acc(y_test, y_pred):\n",
    "    return np.mean(y_test == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of LR on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights\n",
    "w = np.array(0.1*np.random.rand(x_train.shape[1]))[np.newaxis].T\n",
    "\n",
    "# Performing logistic regression training\n",
    "reg = LogisticRegression(w)\n",
    "f = reg.fit(x_train,y_train,0.001,20000, 0.001)\n",
    "\n",
    "# Predicting results on test dataset\n",
    "y_pred = reg.predict(x_test)\n",
    "\n",
    "# Evaluating accuracy\n",
    "lr_init_features = evaluate_acc(y_test, y_pred)*100\n",
    "print(\"Accuracy of LR on test dataset: \",lr_init_features, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Cross Entropy loss from validation on Test dataset\n",
    "plt.title('loss function plot-Breast Cancer')\n",
    "plt.xlabel('iterations'), plt.ylabel('loss function')\n",
    "plt.plot(f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linear discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of LDA in a class\n",
    "class LDA():\n",
    "    \"\"\"\n",
    "    Class to that implements LDA from scratch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializing the required libraries to null.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x_train, y_train, alpha=None, iters=None, stop_criteria=None):\n",
    "        \"\"\"\n",
    "        Function learns the parameters of the LDA model.\n",
    "        Input features: x_train, input labels: y_train\n",
    "        \"\"\"\n",
    "        self.Py0 = np.sum(y_train==0)/y_train.shape[0]\n",
    "        self.Py1 = np.sum(y_train==1)/y_train.shape[0]\n",
    "        self.mu0 = x_train.T @ (y_train==0) / np.sum(y_train==0)\n",
    "        self.mu1 = x_train.T @ (y_train==1) / np.sum(y_train==1)\n",
    "        self.sigma = np.zeros((x_train.shape[1], x_train.shape[1]))\n",
    "        \n",
    "        for i in range(x_train.shape[0]):\n",
    "            self.sigma = self.sigma + (y_train[i]==0) * np.outer(x_train[i]-self.mu0,x_train[i]-self.mu0)\n",
    "            self.sigma = self.sigma + (y_train[i]==1) * np.outer(x_train[i]-self.mu1,x_train[i]-self.mu1)\n",
    "        self.sigma = self.sigma / (y_train.shape[0]-2)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def predict(self, Xtest):\n",
    "        \"\"\"\n",
    "        Function that takes training features as input\n",
    "        \"\"\"\n",
    "        self.w0 = np.log(self.Py1) - np.log(self.Py0) \\\n",
    "             + 0.5 * (np.expand_dims(self.mu0, axis=1).T @ np.linalg.inv(self.sigma) @ np.expand_dims(self.mu0, axis=1)) \\\n",
    "             - 0.5 * (np.expand_dims(self.mu1, axis=1).T @ np.linalg.inv(self.sigma) @ np.expand_dims(self.mu1, axis=1))\n",
    "        self.w1 = np.linalg.inv(self.sigma) @ (self.mu1-self.mu0)\n",
    "        y = self.w0 + Xtest @ self.w1\n",
    "        y = y>0\n",
    "        return y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of LDA using Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "w = np.array(0.1*np.random.rand(x_train.shape[1]))[np.newaxis].T\n",
    "\n",
    "# Perform LDA Training\n",
    "lda = LDA()\n",
    "f = lda.fit(x_train,y_train)\n",
    "\n",
    "# Predicting results on test dataset\n",
    "y_pred = lda.predict(x_test)\n",
    "\n",
    "# Evaluating accuracy on test dataset\n",
    "lda_init_features = evaluate_acc(y_test, y_pred)*100\n",
    "print(\"Accuracy of LDA on test dataset: \", lda_init_features, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Running the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage of cross_validation:\n",
    "#   cross_validation(Reg, 4, x_train, y_train, 0.003, 200)\n",
    "\n",
    "def cross_validation(Reg, k, x_train, y_train, alpha=None, iters=None, stop_criteria=None):\n",
    "    \"\"\"\n",
    "    To perform cross validation\n",
    "     Inputs:\n",
    "        Reg : Regressor term\n",
    "        k : Number of folds\n",
    "        x_train : Training input data\n",
    "        y_train : Training target data\n",
    "        alpha : For Logistic Regression, Learning rate. Otherwise None.\n",
    "        iters: For Logistic Regression, Number of iterations. Otherwise None.\n",
    "        stop_criteria: For Logistic Regression, Stopping criteria. Otherwise None.\n",
    "     Outputs:\n",
    "        Accuracy on cross validation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of samples (approx) in each fold\n",
    "    fold_size = int(len(x_train)/k)\n",
    "    \n",
    "    # accuracies from different folds\n",
    "    cross_validation_accuracies = 0\n",
    "    \n",
    "    # Evaluate for each fold\n",
    "    for fold_number in range(k):\n",
    "        \n",
    "        # Index of start of fold\n",
    "        val_start = fold_number*fold_size\n",
    "        \n",
    "        # Index of end of fold\n",
    "        val_end = val_start + fold_size\n",
    "        \n",
    "        # For last fold, consider validation data from start of last fold till end of data \n",
    "        # (may include more than approx fold size)\n",
    "        # For other folds, evaluate from start of the fold\n",
    "        if fold_number == (k-1):\n",
    "            # Validation data for last fold\n",
    "            x_val_fold = x_train[val_start:,:]\n",
    "            y_val_fold = y_train[val_start:]\n",
    "            \n",
    "            # Training data for other folds\n",
    "            x_train_fold = x_train[:val_start,:]\n",
    "            y_train_fold = y_train[:val_start]\n",
    "        else:\n",
    "            # Validation data for Nth fold\n",
    "            x_val_fold = x_train[val_start:val_end,:]\n",
    "            y_val_fold = y_train[val_start:val_end]\n",
    "            \n",
    "            # Training data for other folds\n",
    "            x_train_fold = np.concatenate((x_train[:val_start,:],x_train[val_end:,:]),axis = 0)\n",
    "            y_train_fold = np.concatenate((y_train[:val_start],y_train[val_end:]))\n",
    "        \n",
    "        # Training on other folds\n",
    "        f = Reg.fit(x_train_fold, y_train_fold, alpha, iters, stop_criteria)\n",
    "        \n",
    "        # Predictions on Validation fold\n",
    "        y_pred_fold = Reg.predict(x_val_fold)\n",
    "        \n",
    "        # Evaluate accuracy for each fold\n",
    "        cross_validation_accuracies += evaluate_acc(y_val_fold, y_pred_fold)\n",
    "        \n",
    "    # Take mean of Validation accuracies in all folds\n",
    "    cross_validation_accuracies /= k\n",
    "    \n",
    "    return cross_validation_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Test different learning rates for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector of learning rates to explore\n",
    "lr_vec = [.0001, .001, 0.003, 0.01, 0.1]\n",
    "\n",
    "# Vector to record Cross validation accuracies\n",
    "cv_acc = []\n",
    "\n",
    "# For each learning rate\n",
    "for lr in lr_vec:\n",
    "    \n",
    "    # Initialize weights\n",
    "    w = np.array(0.1*np.random.rand(x_train.shape[1]))[np.newaxis].T\n",
    "    \n",
    "    # Create LR model, perform cross validation and update Cross validation vector\n",
    "    reg = LogisticRegression(w)\n",
    "    cv_acc.append(cross_validation(reg, 5, x_train, y_train, lr, 20, 0.001))\n",
    "    \n",
    "# Plot log scale of Cross validation accuracies\n",
    "plt.plot(np.log10(lr_vec), cv_acc, '.-')\n",
    "plt.title('cross validation accuracy as learning rate changes')\n",
    "plt.xlabel('learning rate (log scale)')\n",
    "plt.ylabel('cross validation accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Experiment 2: Compare the runtime and accuracy of LDA and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute Cross validation 5 times on LR\n",
    "for i in range(5):\n",
    "    w = np.array(0.1*np.random.rand(x_train.shape[1]))[np.newaxis].T\n",
    "    reg = LogisticRegression(w)\n",
    "    Acc=cross_validation(reg, 5, x_train, y_train, lr, 20, 0.0001)\n",
    "\n",
    "print('Time to run logistic regression 5 times: {} seconds'.format(time.time()-start_time))\n",
    "print('The average accuracy after k-fold cross validation is {} %'.format(Acc*100))\n",
    "print(\"\\n\")\n",
    "# Reset start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute Cross validation 5 times on LDA\n",
    "for i in range(5):\n",
    "    reg = LDA()\n",
    "    ACC=cross_validation(reg, 5, x_train, y_train)\n",
    "\n",
    "print('Time to run LDA 5 times: {} seconds'.format(time.time()-start_time))\n",
    "print('The average accuracy after k-fold cross validation is {} %'.format(ACC*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracies on Test dataset: \")\n",
    "print(\"Logistic Regression: \", lr_init_features, \"%\")\n",
    "print(\"Linear Discriminant Analysis: \", lda_init_features, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
